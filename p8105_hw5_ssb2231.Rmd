---
title: "P8105 Homework 5"
author: "Satya Batna"
output: github_document
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE
)

library(tidyverse)
library(broom)
```
---

## Problem 1

```{r,  echo = FALSE , message = TRUE, warning = FALSE}
birthday_shared <- function(n_people) {
  birthdays <- sample(1:365, size = n_people, replace = TRUE)
  any(duplicated(birthdays))
}

set.seed(1) 

n_sims <- 10000

birthday_results <- 
  tibble(group_size = 2:50) |>
  mutate(
    shared_prob = map_dbl(
      group_size,
      ~ mean(replicate(n_sims, birthday_shared(.x)))
    )
  )

birthday_results |>
  ggplot(aes(x = group_size, y = shared_prob)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Estimated Probability of Shared Birthday by Group Size",
    x = "Group size",
    y = "Probability"
  ) +
  theme_minimal()
```

The resulting plot shows a nonlinear increase in the probability as group size grows. When groups are small,  the probability of a shared birthday is low. The probability starts increasing more noticeably around group sizes 15â€“20 people, and by approximately 23 people the probability reaches about 50 percent. For groups of 50, the probability is nearly 100 percent so it is almost gaurannteeed that someone will share birthdays in this simualtion. 

## Problem 2
```{r problem2, echo = FALSE, message = TRUE, warning = FALSE}
set.seed(1)

# Set Numbers 
n       <- 30
sigma   <- 5
mu_vals <- 0:6
n_sims  <- 5000

# One-sample t-test
sim_results_list <- lapply(mu_vals, function(mu) {
  
  sims <- replicate(
    n_sims,
    {
      x <- rnorm(n, mean = mu, sd = sigma)
      test_out <- t.test(x, mu = 0)
      
      c(
        estimate = mean(x),
        p_value  = test_out$p.value
      )
    },
    simplify = TRUE
  )
  
  tibble(
    mu_true  = mu,
    estimate = sims["estimate", ],
    p_value  = sims["p_value", ]
  )
})

sim_results <- bind_rows(sim_results_list)

# Power
power_df <- 
  sim_results |>
  group_by(mu_true) |>
  summarize(
    power = mean(p_value < 0.05),
    .groups = "drop"
  )

power_df

power_plot <- 
  power_df |>
  ggplot(aes(x = mu_true, y = power)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 1)) +
  labs(
    title = "Power of one-sample t-test by true mean",
    x = "True mean (mu)",
    y = "Proportion of rejections (power)"
  ) +
  theme_minimal()

power_plot

# Average estimates
estimate_df <- 
  sim_results |>
  group_by(mu_true) |>
  summarize(
    mean_est_all = mean(estimate),
    mean_est_sig = mean(estimate[p_value < 0.05]),
    .groups = "drop"
  )

estimate_df

estimate_plot <- 
  estimate_df |>
  ggplot(aes(x = mu_true)) +
  geom_line(aes(y = mean_est_all), linetype = "solid") +
  geom_point(aes(y = mean_est_all)) +
  geom_line(aes(y = mean_est_sig), linetype = "dashed") +
  geom_point(aes(y = mean_est_sig)) +
  labs(
    title = "Average estimate of mu by True mu",
    x = "True mean (mu)",
    y = "Average estimate",
    caption = "Solid: all simulations; dashed: only where H0 rejected"
  ) +
  theme_minimal()

estimate_plot
```
1. Power vs. effect size

The plot with Power shows that when the true mean is zero, the null is rejected about 0.05 or (5% of the time). As effect size increases, the proportion of rejections rises too. This shows us that the larger effect sizes the higher the power.

2. Average estimate vs. true mean

The plot shows that the average estimate across all simulations is very close to the true mean, which means the sample mean is unbiased, But, when looking only at simulations where the null hypothesis was rejected, the average estimate is higher than the true mean, especially when the true mean is small. This happens because we are only keeping the samples that are significant, which causes the estimates in that group to be higher. 

```{r, echo = FALSE, message = TRUE, warning = FALSE}
library(purrr)

# ------------------------------------------------------------------------
# Load raw data
# ------------------------------------------------------------------------

homicides =
  read_csv("data/homicide-data.csv")

#Data organization 

homicide_summary =
  homicides |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
  ) |>
  group_by(city_state) |>
  summarize(
    total_homicides    = n(),
    unsolved_homicides = sum(unsolved),
    .groups = "drop"
  )

homicide_summary

# Only Baltimore Testing

baltimore_summary =
  homicide_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_summary

baltimore_test =
  prop.test(
    x = baltimore_summary$unsolved_homicides,
    n = baltimore_summary$total_homicides
  )

baltimore_tidy = tidy(baltimore_test)
baltimore_tidy

#For the other cities too
city_prop_results =
  homicide_summary |>
  mutate(
    prop_test = map2(
      unsolved_homicides,
      total_homicides,
      \(x, n) prop.test(x, n)
    ),
    prop_tidy = map(prop_test, tidy)
  ) |>
  unnest(prop_tidy) |>
  select(
    city_state,
    total_homicides,
    unsolved_homicides,
    estimate,
    conf.low,
    conf.high
  )

city_prop_results

# Plot

city_plot =
  city_prop_results |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  coord_flip() +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion unsolved"
  ) +
  theme_minimal()

city_plot
```
Chicago, IL seem to have the most unsolved cases, probably why every crime show takes place there. Tulsa, AL has the least unsolved cases. 
